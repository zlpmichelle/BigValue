# define mode
dataload.mode=local
dataload.source.streaming.fetch=false
buildIndex=false

# define data source
dataload.source.dataSourceClass=com.cloudera.bigdata.analysis.dataload.source.HDFSDataSource
dataload.source.hdfsDirs=hdfs://vt-nn:8020/example23g
#dataload.source.hdfsDirs=hdfs://vt-nn:8020/example21g

# define data format
dataload.source.parserType=csv
dataload.source.instanceDocPath=../conf/ftpOrHdfs2hbase.xml

# define client for mapper
dataload.client.queueLength=40000
dataload.client.fetchParallel=60
dataload.client.threadsPerMapper=10

# define target table
server.hbase.coprocessor.location=hdfs://vt-nn:8020/user/example1/BigValue-1.0.jar
server.search.enableStatistics=false
useHashUUIDForRowkey=true
regionQuantity=60
hbase.table.useInMemory=
hbase.target.table.name=test
#hbase.table.splitKeyPrefixes=16163001,16163009,16163026,16163027,16163030,16163089,16163098,16163118,16163182
hbase.table.splitSize=100
hbase.table.writeToWAL=false
hbase.table.writeBufferSize=6
hbase.table.autoFlush=false
hbase.table.columnReplication=1
hbase.table.memStore=64

